{
    "name": "FFHQ_ppo",
    "phase": "train", // train or val
    "gpu_ids": [
        1
    ],
    "threshold": 0.0,
    "path": { //set the path
        "log": "logs",
        "tb_logger": "tb_logger",
        "results": "results",
        "checkpoint": "checkpoint",
        "resume": 0,
        "resume_state": "/home/xxx/pretrain/FFHQ_pretrain/model"
    },
    "datasets": {
        "train": {
            "name": "FFHQ",  // FFHQ xiaotongtt DIV2K
            "scale": 8,
            "dataroot_HR": "/home/xx/FFHQ/hr_128/",
            "dataroot_LR": "/home/xx/FFHQ/lr_16/",
            "subset_file": null,
            "phase": "train",
            "mode": "LRHR", // whether need LR img HR
            "data_type": "img", //lmdb or img, path of img files  lmdb
            "l_resolution": 16, // low resolution need to super_resolution
            "r_resolution": 128, // high resolution
            "batch_size": 4,  // 4 16
            "num_workers": 8,
            "use_shuffle": true,
            "data_len": -1, // -1 represents all data used in train
            "HR_size": 128, // 128 | 192 160
            "use_flip": true,
            "use_rot": true
        },
        "val": {
            "name": "CelebA-HQ-val",
            "mode": "LRHR",
            "phase": "val",
            "dataroot_HR": "/home/xx/celeba_hq_val_1000/hr_128/",
            "dataroot_LR": "/home/xx/celeba_hq_val_1000/lr_16/",
            "data_type": "img", //lmdb or img, path of img files  lmdb
            "scale": 8,
            "l_resolution": 16,
            "r_resolution": 128,
            "data_len": 1000 // data length in validation
        }
    },
    "model": {
        "which_model_G": "sr3", // use the ddpm or sr3 network structure
        "which_model_backbone": "unet",
        "finetune_norm": false,
        "unet": {
            "in_channel": 6,
            "out_channel": 3,
            "inner_channel": 64,
            "channel_multiplier": [
                1,
                2,
                4,
                8,
                8
            ],
            "attn_res": [
                16
            ],
            "res_blocks": 2,
            "dropout": 0.2
        },
        "beta_schedule": { // use munual beta_schedule for acceleration
            "train": {
                "schedule": "cosine",  // linear
                "n_timestep": 100, //2000 1000 10 100
                "linear_start": 1e-6,
                "linear_end": 1e-2
            },
            "val": {
                "schedule": "cosine", // linear
                "n_timestep": 100, //2000 1000 10
                "linear_start": 1e-6,
                "linear_end": 1e-2
            }
        },
        "diffusion": {
            "image_size": 128, //128
            "channels": 3, //sample channel
            "conditional": true
        }
    },
    "train": {
        "n_iter": 50000,
        "val_freq": 1e3,
        "save_checkpoint_freq": 1e3,
        "print_freq": 200,  // 200
        "optimizer": {
            "type": "adam",
            "lr": 2e-4,
            "beta1": 0.9,
            "beta2": 0.99,
            "lr_scheme": "step",
            "lr_decay": 50000,
            "lr_gamma": 0.5,
            "eta_min": 1e-7 //!!float
        },
        "ema_scheduler": {
            "step_start_ema": 5000,
            "update_ema_every": 1,
            "ema_decay": 0.9999
        }
    },
    "wandb": {
        "project": "sr_ffhq"
    }
}